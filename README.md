# Stanford CME 241: Self-Study Journey ðŸ“š

### Introduction
This repository documents my self-study journey through Stanford's CME 241 course (Reinforcement Learning for Stochastic Control Problems in Finance) based on the textbook "Foundations of Reinforcement Learning with Applications in Finance" by Ashwin Rao and Tikhon Jelvis.



### Course Focus
The course explores the intersection of Reinforcement Learning and Financial Applications, providing a comprehensive foundation in:
* Markov Decision Processes (MDPs)
* Dynamic Programming Algorithms
* Reinforcement Learning Algorithms
* Financial Trading Applications



### Learning Objectives
#### Theoretical Foundations
* Master Markov Decision Processes (MDPs) framework
* Understand Bellman Equations and their applications
* Study Dynamic Programming algorithms including Policy Iteration and * Value Iteration

#### Reinforcement Learning Algorithms
* Basic algorithms: SARSA and Q-Learning
* Advanced techniques:
   * Gradient Temporal Difference
   * Deep Q-Networks
   * Least-Squares Policy Iteration
   * Policy Gradient
   * Monte-Carlo Tree Search

#### Financial Applications
* Dynamic Asset-Allocation
* Derivatives Pricing and Hedging
* American Options Optimal Exercise
* Trade Order Execution
* Market-Making Strategies



### Implementation Focus
All implementations are done "from scratch" in Python, emphasizing:
* Clean interface design
* Type annotations
* Functional programming principles
* Inheritance-based polymorphism



### Acknowledgments
This project is heavily inspired by Ashwin Rao and Tikhon Jelvisâ€™s textbook, Foundations of Reinforcement Learning with Applications in Finance. Concepts and examples from the book form the basis for these implementations, although all code and commentary reflect my understanding and personal learning progress.
